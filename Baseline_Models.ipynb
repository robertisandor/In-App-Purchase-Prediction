{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:05:52.511879Z",
     "start_time": "2019-02-27T05:05:51.979762Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import calendar\n",
    "import boto3\n",
    "import io\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T01:29:09.769635Z",
     "start_time": "2019-02-25T01:29:09.765238Z"
    }
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:05:53.118096Z",
     "start_time": "2019-02-27T05:05:52.638810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_hash</th>\n",
       "      <th>user_purchase_binary_7_days</th>\n",
       "      <th>user_purchase_binary_14_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e469dfaed039ead9110165d9bc457acb11609ca34057dc...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id_hash  \\\n",
       "0  e469dfaed039ead9110165d9bc457acb11609ca34057dc...   \n",
       "\n",
       "   user_purchase_binary_7_days  user_purchase_binary_14_days  \n",
       "0                         0.01                          0.02  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub_df = pd.read_csv('data/sample_submission_2_adj.csv').drop([\"Unnamed: 0\"], axis = 1)\n",
    "sample_sub_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:05:53.590163Z",
     "start_time": "2019-02-27T05:05:53.372565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_hash</th>\n",
       "      <th>user_purchase_binary_7_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bf676611754201cc93ca1e2bcce8ca2c7ff0105186ebc0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id_hash  \\\n",
       "0  bf676611754201cc93ca1e2bcce8ca2c7ff0105186ebc0...   \n",
       "\n",
       "   user_purchase_binary_7_days  \n",
       "0                          1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read labels\n",
    "label_1_df = pd.read_csv(\"data/label_1.csv\", header = None)\\\n",
    "               .rename(columns = {0:\"user_id_hash\", \n",
    "                                  1:\"user_purchase_binary_7_days\"})\n",
    "label_2_df = pd.read_csv(\"data/label_2.csv\", header = None)\\\n",
    "               .rename(columns = {0:\"user_id_hash\", \n",
    "                                  1:\"user_purchase_binary_14_days\"})\n",
    "label_1_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary for Total\n",
    "0 - user_id  \n",
    "1 - total number of sessions  \n",
    "2 - total number of events  \n",
    "3 - median number of events  \n",
    "4 - total sum of purchases  \n",
    "5 - total amount of purchases  \n",
    "6 - median session duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:05:56.157209Z",
     "start_time": "2019-02-27T05:05:53.848716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_hash</th>\n",
       "      <th>num_sessions</th>\n",
       "      <th>num_events</th>\n",
       "      <th>median_num_events</th>\n",
       "      <th>num_purchases</th>\n",
       "      <th>amt_purchases</th>\n",
       "      <th>median_session_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0d261313961125c71f330a8a8d59857bc48fefd0b4278c...</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1626343.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id_hash  num_sessions  \\\n",
       "0  0d261313961125c71f330a8a8d59857bc48fefd0b4278c...             1   \n",
       "\n",
       "   num_events  median_num_events  num_purchases  amt_purchases  \\\n",
       "0          35               35.0              0            0.0   \n",
       "\n",
       "   median_session_duration  \n",
       "0                1626343.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Total\n",
    "total_train_df = pd.read_csv(\"data/train_total.csv\", header = None)\\\n",
    "                   .rename(columns = {0:\"user_id_hash\", \n",
    "                                      1:\"num_sessions\",\n",
    "                                      2:\"num_events\",\n",
    "                                      3:\"median_num_events\",\n",
    "                                      4:\"num_purchases\",\n",
    "                                      5:\"amt_purchases\",\n",
    "                                      6:\"median_session_duration\"})\n",
    "total_test_df = pd.read_csv(\"data/test_total.csv\", header = None)\\\n",
    "                  .rename(columns = {0:\"user_id_hash\", \n",
    "                                     1:\"num_sessions\",\n",
    "                                     2:\"num_events\",\n",
    "                                     3:\"median_num_events\",\n",
    "                                     4:\"num_purchases\",\n",
    "                                     5:\"amt_purchases\",\n",
    "                                     6:\"median_session_duration\"})\n",
    "total_train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:05:56.533683Z",
     "start_time": "2019-02-27T05:05:56.160276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_hash</th>\n",
       "      <th>num_sessions_7</th>\n",
       "      <th>num_events_7</th>\n",
       "      <th>median_num_events_7</th>\n",
       "      <th>num_purchases_7</th>\n",
       "      <th>amt_purchases_7</th>\n",
       "      <th>median_session_duration_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c7385007b6f158e913411d1563d1f5ef90ea003050b105...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id_hash  num_sessions_7  \\\n",
       "0  c7385007b6f158e913411d1563d1f5ef90ea003050b105...               1   \n",
       "\n",
       "   num_events_7  median_num_events_7  num_purchases_7  amt_purchases_7  \\\n",
       "0             1                  1.0                0              0.0   \n",
       "\n",
       "   median_session_duration_7  \n",
       "0                        0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Last 7 Days\n",
    "train_7_df = pd.read_csv(\"feature_eng/train_last_7_days.csv\", header = None)\\\n",
    "                   .rename(columns = {0:\"user_id_hash\", \n",
    "                                      1:\"num_sessions_7\",\n",
    "                                      2:\"num_events_7\",\n",
    "                                      3:\"median_num_events_7\",\n",
    "                                      4:\"num_purchases_7\",\n",
    "                                      5:\"amt_purchases_7\",\n",
    "                                      6:\"median_session_duration_7\"})\n",
    "test_7_df = pd.read_csv(\"feature_eng/test_last_7_days.csv\", header = None)\\\n",
    "                  .rename(columns = {0:\"user_id_hash\", \n",
    "                                     1:\"num_sessions_7\",\n",
    "                                     2:\"num_events_7\",\n",
    "                                     3:\"median_num_events_7\",\n",
    "                                     4:\"num_purchases_7\",\n",
    "                                     5:\"amt_purchases_7\",\n",
    "                                     6:\"median_session_duration_7\"})\n",
    "train_7_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:05:57.085956Z",
     "start_time": "2019-02-27T05:05:56.535229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_hash</th>\n",
       "      <th>num_sessions_14</th>\n",
       "      <th>num_events_14</th>\n",
       "      <th>median_num_events_14</th>\n",
       "      <th>num_purchases_14</th>\n",
       "      <th>amt_purchases_14</th>\n",
       "      <th>median_session_duration_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a7d68ea7dcf1bc9fc2d455c19d1ba409951609735a543b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id_hash  num_sessions_14  \\\n",
       "0  a7d68ea7dcf1bc9fc2d455c19d1ba409951609735a543b...                1   \n",
       "\n",
       "   num_events_14  median_num_events_14  num_purchases_14  amt_purchases_14  \\\n",
       "0              1                   1.0                 0               0.0   \n",
       "\n",
       "   median_session_duration_14  \n",
       "0                         0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Last 14 Days\n",
    "train_14_df = pd.read_csv(\"feature_eng/train_last_14_days.csv\", header = None)\\\n",
    "                   .rename(columns = {0:\"user_id_hash\", \n",
    "                                      1:\"num_sessions_14\",\n",
    "                                      2:\"num_events_14\",\n",
    "                                      3:\"median_num_events_14\",\n",
    "                                      4:\"num_purchases_14\",\n",
    "                                      5:\"amt_purchases_14\",\n",
    "                                      6:\"median_session_duration_14\"})\n",
    "test_14_df = pd.read_csv(\"feature_eng/test_last_14_days.csv\", header = None)\\\n",
    "                  .rename(columns = {0:\"user_id_hash\", \n",
    "                                     1:\"num_sessions_14\",\n",
    "                                     2:\"num_events_14\",\n",
    "                                     3:\"median_num_events_14\",\n",
    "                                     4:\"num_purchases_14\",\n",
    "                                     5:\"amt_purchases_14\",\n",
    "                                     6:\"median_session_duration_14\"})\n",
    "train_14_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:54:06.786896Z",
     "start_time": "2019-02-25T02:54:06.769823Z"
    }
   },
   "source": [
    "### Data Dictionary for Sessions\n",
    "0 - user_id  \n",
    "1 - date_user_created   \n",
    "2 - most_freq_country  \n",
    "3 - most_freq_os  \n",
    "4 - num_uniq_device_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:05:58.585619Z",
     "start_time": "2019-02-27T05:05:57.088797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_hash</th>\n",
       "      <th>date_user_created</th>\n",
       "      <th>most_freq_country</th>\n",
       "      <th>most_freq_os</th>\n",
       "      <th>num_uniq_device_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46d54b4fab292c461cdf8f3825f7106907b300b2d28cf3...</td>\n",
       "      <td>2018-10-19 22:04:58</td>\n",
       "      <td>US</td>\n",
       "      <td>iOS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id_hash    date_user_created  \\\n",
       "0  46d54b4fab292c461cdf8f3825f7106907b300b2d28cf3...  2018-10-19 22:04:58   \n",
       "\n",
       "  most_freq_country most_freq_os  num_uniq_device_id  \n",
       "0                US          iOS                   1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Sessions\n",
    "sessions_df = pd.read_csv(\"feature_eng/sessions.csv\", header = None)\\\n",
    "                   .rename(columns = {0:\"user_id_hash\", \n",
    "                                      1:\"date_user_created\",\n",
    "                                      2:\"most_freq_country\",\n",
    "                                      3:\"most_freq_os\",\n",
    "                                      4:\"num_uniq_device_id\"})\\\n",
    "                   .dropna()\n",
    "sessions_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:05.569857Z",
     "start_time": "2019-02-27T05:05:58.588055Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fe = pd.merge(total_train_df, sessions_df, how = \"left\").dropna()\n",
    "train_fe = pd.merge(train_fe, train_7_df, how = \"left\").fillna(0)\n",
    "train_fe = pd.merge(train_fe, train_14_df, how = \"left\").fillna(0)\n",
    "test_fe = pd.merge(total_test_df, sessions_df, how = \"left\").dropna()\n",
    "test_fe = pd.merge(test_fe, test_7_df, how = \"left\").fillna(0)\n",
    "test_fe = pd.merge(test_fe, test_14_df, how = \"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:06.115289Z",
     "start_time": "2019-02-27T05:06:05.572629Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check null values\n",
    "for column in train_fe.columns:\n",
    "    if train_fe[column].isnull().any():\n",
    "        print('{0} has {1} null values'.format(column, train_fe[column].isnull().sum()))\n",
    "for column in test_fe.columns:\n",
    "    if test_fe[column].isnull().any():\n",
    "        print('{0} has {1} null values'.format(column, test_fe[column].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create column for Number of Days Existed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:18.543853Z",
     "start_time": "2019-02-27T05:06:06.117159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create column for Number of Days Existed\n",
    "# For training, compute days between first date and Dec.2\n",
    "# For testing, compute days between first date and Dec.16\n",
    "train_fe[\"num_days_existed\"] = [(datetime(2018, 12, 2) - datetime.strptime(date, '%Y-%m-%d %H:%M:%S')).days \n",
    "                                for date in train_fe.date_user_created.values]\n",
    "test_fe[\"num_days_existed\"] = [(datetime(2018, 12, 16) - datetime.strptime(date, '%Y-%m-%d %H:%M:%S')).days \n",
    "                                for date in test_fe.date_user_created.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create column for Days Since Most Recent Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:21.276827Z",
     "start_time": "2019-02-27T05:06:18.545772Z"
    }
   },
   "outputs": [],
   "source": [
    "most_recent_train_df = pd.read_csv('feature_eng/most_recent_session_train.csv', header = None)\\\n",
    "                         .rename(columns = {0:\"user_id_hash\", 1:\"most_recent_session\"})\n",
    "most_recent_test_df = pd.read_csv('feature_eng/most_recent_session_test.csv', header = None)\\\n",
    "                        .rename(columns = {0:\"user_id_hash\", 1:\"most_recent_session\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:23.738546Z",
     "start_time": "2019-02-27T05:06:21.279790Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fe_adj = pd.merge(train_fe, most_recent_train_df, how = 'left')\n",
    "test_fe_adj = pd.merge(test_fe, most_recent_test_df, how = 'left')\n",
    "train_fe_adj2 = train_fe_adj\n",
    "test_fe_adj2 = test_fe_adj\n",
    "train_fe_adj2[\"most_recent_session\"] = train_fe_adj[\"most_recent_session\"].fillna(train_fe_adj[\"date_user_created\"])\n",
    "test_fe_adj2[\"most_recent_session\"] = test_fe_adj[\"most_recent_session\"].fillna(test_fe_adj[\"date_user_created\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:36.219336Z",
     "start_time": "2019-02-27T05:06:23.745092Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fe[\"days_since_last_session\"] = [(datetime(2018, 12, 2) - datetime.strptime(date, '%Y-%m-%d %H:%M:%S')).days \n",
    "                                        for date in train_fe_adj2.most_recent_session.values]\n",
    "test_fe[\"days_since_last_session\"] = [(datetime(2018, 12, 16) - datetime.strptime(date, '%Y-%m-%d %H:%M:%S')).days \n",
    "                                        for date in test_fe_adj2.most_recent_session.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create column for frequent OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:36.756257Z",
     "start_time": "2019-02-27T05:06:36.222236Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fe[\"most_freq_os_android\"] = [1 if x == 'Android OS' else 0 for x in train_fe.most_freq_os.values]\n",
    "train_fe[\"most_freq_os_ios\"] = [1 if x != 'Android OS' else 0 for x in train_fe.most_freq_os.values]\n",
    "test_fe[\"most_freq_os_android\"] = [1 if x == 'Android OS' else 0 for x in test_fe.most_freq_os.values]\n",
    "test_fe[\"most_freq_os_ios\"] = [1 if x != 'Android OS' else 0 for x in test_fe.most_freq_os.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummies regarding regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:36.869803Z",
     "start_time": "2019-02-27T05:06:36.758728Z"
    }
   },
   "outputs": [],
   "source": [
    "country_code_df = pd.read_csv(\"country_code.csv\", header = None)\n",
    "country_code_df = country_code_df.loc[1:,[1,5,6]]\n",
    "country_code_df = country_code_df[country_code_df[1] != 'AQ'].dropna()\n",
    "country_code_dict = {}\n",
    "country_code_dict['ZZ'] = \"Unknown\"\n",
    "country_code_dict['XK'] = \"Southern Europe\"\n",
    "country_code_dict['NAM'] = \"Sub-Saharan Africa\"\n",
    "for country_code in country_code_df[1].values:\n",
    "    country_code_dict[country_code] = country_code_df[country_code_df[1]==country_code][6].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:36.987704Z",
     "start_time": "2019-02-27T05:06:36.872208Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fe[\"most_freq_region\"] = [country_code_dict[x] for x in train_fe.most_freq_country.values]\n",
    "test_fe[\"most_freq_region\"] = [country_code_dict[x] for x in test_fe.most_freq_country.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:37.381118Z",
     "start_time": "2019-02-27T05:06:36.989409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Southern Asia 57538\n",
      "Sub-Saharan Africa 23872\n",
      "Western Europe 21095\n",
      "Latin America and the Caribbean 24213\n",
      "Australia and New Zealand 20900\n",
      "Northern America 297832\n",
      "South-eastern Asia 55423\n",
      "Northern Europe 60393\n"
     ]
    }
   ],
   "source": [
    "region_list = []\n",
    "for region in set(train_fe.most_freq_region.values):\n",
    "    if list(train_fe.most_freq_region.values).count(region) > 20000:\n",
    "        print(region, list(train_fe.most_freq_region.values).count(region))\n",
    "        region_list.append(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:39.518402Z",
     "start_time": "2019-02-27T05:06:37.382933Z"
    }
   },
   "outputs": [],
   "source": [
    "for region in region_list:\n",
    "    col_name = f\"is_{region.replace(' ','_').lower()}\"\n",
    "    train_fe[col_name] = [1 if x == region else 0 for x in train_fe.most_freq_region.values]\n",
    "    test_fe[col_name] = [1 if x == region else 0 for x in test_fe.most_freq_region.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:39.544146Z",
     "start_time": "2019-02-27T05:06:39.520200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_hash</th>\n",
       "      <th>num_sessions</th>\n",
       "      <th>num_events</th>\n",
       "      <th>median_num_events</th>\n",
       "      <th>num_purchases</th>\n",
       "      <th>amt_purchases</th>\n",
       "      <th>median_session_duration</th>\n",
       "      <th>date_user_created</th>\n",
       "      <th>most_freq_country</th>\n",
       "      <th>most_freq_os</th>\n",
       "      <th>...</th>\n",
       "      <th>most_freq_os_ios</th>\n",
       "      <th>most_freq_region</th>\n",
       "      <th>is_southern_asia</th>\n",
       "      <th>is_sub-saharan_africa</th>\n",
       "      <th>is_western_europe</th>\n",
       "      <th>is_latin_america_and_the_caribbean</th>\n",
       "      <th>is_australia_and_new_zealand</th>\n",
       "      <th>is_northern_america</th>\n",
       "      <th>is_south-eastern_asia</th>\n",
       "      <th>is_northern_europe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0d261313961125c71f330a8a8d59857bc48fefd0b4278c...</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1626343.0</td>\n",
       "      <td>2018-10-13 07:33:07</td>\n",
       "      <td>ID</td>\n",
       "      <td>iOS</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>South-eastern Asia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id_hash  num_sessions  \\\n",
       "0  0d261313961125c71f330a8a8d59857bc48fefd0b4278c...             1   \n",
       "\n",
       "   num_events  median_num_events  num_purchases  amt_purchases  \\\n",
       "0          35               35.0              0            0.0   \n",
       "\n",
       "   median_session_duration    date_user_created most_freq_country  \\\n",
       "0                1626343.0  2018-10-13 07:33:07                ID   \n",
       "\n",
       "  most_freq_os         ...          most_freq_os_ios    most_freq_region  \\\n",
       "0          iOS         ...                         1  South-eastern Asia   \n",
       "\n",
       "   is_southern_asia  is_sub-saharan_africa  is_western_europe  \\\n",
       "0                 0                      0                  0   \n",
       "\n",
       "   is_latin_america_and_the_caribbean  is_australia_and_new_zealand  \\\n",
       "0                                   0                             0   \n",
       "\n",
       "   is_northern_america  is_south-eastern_asia  is_northern_europe  \n",
       "0                    0                      1                   0  \n",
       "\n",
       "[1 rows x 36 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fe.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:42.686241Z",
     "start_time": "2019-02-27T05:06:39.546093Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fe = pd.merge(train_fe, label_1_df, how = 'left').fillna(0)\n",
    "train_fe = pd.merge(train_fe, label_2_df, how = 'left').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:43.064043Z",
     "start_time": "2019-02-27T05:06:42.688606Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ml = train_fe.copy()\n",
    "test_ml = test_fe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:43.076321Z",
     "start_time": "2019-02-27T05:06:43.066115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the labels are correct. The expected outcome is {0.0}.\n",
    "set(train_ml[train_ml.user_purchase_binary_7_days != train_ml.user_purchase_binary_14_days]\\\n",
    " .user_purchase_binary_7_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:43.083597Z",
     "start_time": "2019-02-27T05:06:43.078411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id_hash', 'num_sessions', 'num_events', 'median_num_events',\n",
       "       'num_purchases', 'amt_purchases', 'median_session_duration',\n",
       "       'date_user_created', 'most_freq_country', 'most_freq_os',\n",
       "       'num_uniq_device_id', 'num_sessions_7', 'num_events_7',\n",
       "       'median_num_events_7', 'num_purchases_7', 'amt_purchases_7',\n",
       "       'median_session_duration_7', 'num_sessions_14', 'num_events_14',\n",
       "       'median_num_events_14', 'num_purchases_14', 'amt_purchases_14',\n",
       "       'median_session_duration_14', 'num_days_existed',\n",
       "       'days_since_last_session', 'most_freq_os_android', 'most_freq_os_ios',\n",
       "       'most_freq_region', 'is_southern_asia', 'is_sub-saharan_africa',\n",
       "       'is_western_europe', 'is_latin_america_and_the_caribbean',\n",
       "       'is_australia_and_new_zealand', 'is_northern_america',\n",
       "       'is_south-eastern_asia', 'is_northern_europe',\n",
       "       'user_purchase_binary_7_days', 'user_purchase_binary_14_days'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ml.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:43.089824Z",
     "start_time": "2019-02-27T05:06:43.086451Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_ml[\"is_purchase_7\"] = [1.0 if y > 0 else 0.0 for y in train_ml.num_purchases_7]\n",
    "# train_ml[\"is_purchase_ever\"] = [1.0 if y > 0 else 0.0 for y in train_ml.num_purchases]\n",
    "# train_ml[\"is_session_7\"] = [1.0 if y > 0 else 0.0 for y in train_ml.num_sessions_7]\n",
    "# train_ml[\"is_purchase_14\"] = [1.0 if y > 0 else 0.0 for y in train_ml.num_purchases_14]\n",
    "# train_ml[\"is_session_14\"] = [1.0 if y > 0 else 0.0 for y in train_ml.num_sessions_14]\n",
    "# train_ml['purchase_per_day'] = train_ml.num_purchases/train_ml.num_days_existed\n",
    "# test_ml[\"is_purchase_7\"] = [1.0 if y > 0 else 0.0 for y in test_ml.num_purchases_7]\n",
    "# test_ml[\"is_purchase_ever\"] = [1.0 if y > 0 else 0.0 for y in test_ml.num_purchases]\n",
    "# test_ml[\"is_session_7\"] = [1.0 if y > 0 else 0.0 for y in test_ml.num_sessions_7]\n",
    "# test_ml[\"is_purchase_14\"] = [1.0 if y > 0 else 0.0 for y in test_ml.num_purchases_14]\n",
    "# test_ml[\"is_session_14\"] = [1.0 if y > 0 else 0.0 for y in test_ml.num_sessions_14]\n",
    "# test_ml['purchase_per_day'] = test_ml.num_purchases/test_ml.num_days_existed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:43.095765Z",
     "start_time": "2019-02-27T05:06:43.092582Z"
    }
   },
   "outputs": [],
   "source": [
    "# # per num_days_existed\n",
    "# train_ml[\"num_uniq_session_db_num_days_existed\"] = train_ml.num_sessions / train_ml.num_days_existed\n",
    "# test_ml[\"num_uniq_session_db_num_days_existed\"] = test_ml.num_sessions / test_ml.num_days_existed\n",
    "# train_ml[\"total_purchase_amt_db_num_days_existed\"] = train_ml.amt_purchases / train_ml.num_days_existed\n",
    "# test_ml[\"total_purchase_amt_db_num_days_existed\"] = test_ml.amt_purchases / test_ml.num_days_existed\n",
    "# # per num_uniq_session_id\n",
    "# train_ml[\"total_purchase_amt_db_num_uniq_session_id\"] = train_ml.amt_purchases / train_ml.num_sessions\n",
    "# test_ml[\"total_purchase_amt_db_num_uniq_session_id\"] = test_ml.amt_purchases / test_ml.num_sessions\n",
    "# train_ml[\"total_purchase_amt_db_total_num_purchase\"] = train_ml.amt_purchases / train_ml.num_purchases\n",
    "# test_ml[\"total_purchase_amt_db_total_num_purchase\"] = test_ml.amt_purchases / test_ml.num_purchases\n",
    "# train_ml = train_ml.fillna(0)\n",
    "# test_ml = test_ml.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:06:43.100526Z",
     "start_time": "2019-02-27T05:06:43.097926Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_ml[\"amt_purchase_past_wk_per_purchase\"] = train_ml.amt_purchases_7 / train_ml.num_purchases_7\n",
    "# test_ml[\"amt_purchase_past_wk_per_purchase\"] = test_ml.amt_purchases_7 / test_ml.num_purchases_7\n",
    "# train_ml[\"amt_purchase_past_wk_per_session\"] = train_ml.amt_purchases_7 / train_ml.num_sessions_7\n",
    "# test_ml[\"amt_purchase_past_wk_per_session\"] = test_ml.amt_purchases_7 / test_ml.num_sessions_7\n",
    "# train_ml = train_ml.fillna(0)\n",
    "# test_ml = test_ml.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:17:46.749484Z",
     "start_time": "2019-02-27T05:17:46.729116Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set labels\n",
    "labels_1 = train_ml.user_purchase_binary_7_days\n",
    "labels_2 = train_ml.user_purchase_binary_14_days\n",
    "features_train = train_ml[['num_purchases','days_since_last_session']]\n",
    "features_test = test_ml[['num_purchases','days_since_last_session']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:17:52.380847Z",
     "start_time": "2019-02-27T05:17:51.376890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF_Factor</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.002986</td>\n",
       "      <td>num_purchases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.002986</td>\n",
       "      <td>days_since_last_session</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VIF_Factor                 features\n",
       "0    1.002986            num_purchases\n",
       "1    1.002986  days_since_last_session"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF_Factor\"] = [variance_inflation_factor(features_train.values, i) for i in range(features_train.shape[1])]\n",
    "vif[\"features\"] = features_train.columns\n",
    "vif.sort_values(by = 'VIF_Factor', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:08:36.893519Z",
     "start_time": "2019-02-27T05:08:36.454415Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:17:56.408488Z",
     "start_time": "2019-02-27T05:17:56.171915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape: Counter({0.0: 4396, 1.0: 4396})\n"
     ]
    }
   ],
   "source": [
    "# Resampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "rus = RandomUnderSampler()\n",
    "features_res_1, labels_res_1 = rus.fit_sample(features_train, labels_1)\n",
    "print(f\"Resampled dataset shape: {Counter(labels_res_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:17:56.414435Z",
     "start_time": "2019-02-27T05:17:56.410658Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(features_res_1, labels_res_1, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:17:56.662678Z",
     "start_time": "2019-02-27T05:17:56.642199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for lr model 1: 0.9583\n",
      "Log Loss for lr model 1: 0.2986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xulian/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_1 = LogisticRegression(class_weight = \"balanced\")\n",
    "lr_1.fit(X_train_1, y_train_1)\n",
    "lr_1_pred = lr_1.predict_proba(X_val_1)[:,1]\n",
    "print(f\"ROC AUC for lr model 1: {roc_auc_score(y_val_1, lr_1_pred):.4f}\")\n",
    "print(f\"Log Loss for lr model 1: {log_loss(y_val_1, lr_1_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:17:58.152648Z",
     "start_time": "2019-02-27T05:17:58.142479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for nb model 1: 0.9276\n",
      "Log Loss for nb model 1: 0.2874\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb_1 = BernoulliNB(fit_prior=True)\n",
    "nb_1.fit(X_train_1, y_train_1)\n",
    "nb_1_pred = nb_1.predict_proba(X_val_1)[:,1]\n",
    "print(f\"ROC AUC for nb model 1: {roc_auc_score(y_val_1, nb_1_pred):.4f}\")\n",
    "print(f\"Log Loss for nb model 1: {log_loss(y_val_1, nb_1_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:19:25.129073Z",
     "start_time": "2019-02-27T05:19:25.087148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for rf model 1: 0.9592\n",
      "Log Loss for rf model 1: 0.4845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xulian/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_1 = RandomForestClassifier()\n",
    "rf_1.fit(X_train_1, y_train_1)\n",
    "rf_1_pred = rf_1.predict_proba(X_val_1)[:,1]\n",
    "print(f\"ROC AUC for rf model 1: {roc_auc_score(y_val_1, rf_1_pred):.4f}\")\n",
    "print(f\"Log Loss for rf model 1: {log_loss(y_val_1, rf_1_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:17:59.070884Z",
     "start_time": "2019-02-27T05:17:59.045662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for xgb model 1: 0.9647\n",
      "Log Loss for xgb model 1: 0.6882\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model_1 = xgb.XGBClassifier(\n",
    " learning_rate =0.001,\n",
    " n_estimators=9,\n",
    " max_depth=6,\n",
    " min_child_weight=1,\n",
    " subsample=0.3,\n",
    " colsample_bytree=0.5,\n",
    " objective= 'binary:logistic')\n",
    "\n",
    "xgb_1 = model_1.fit(X_train_1, y_train_1)\n",
    "xgb_1_pred = xgb_1.predict_proba(X_val_1)[:,1]\n",
    "print(f\"ROC AUC for xgb model 1: {roc_auc_score(y_val_1, xgb_1_pred):.4f}\")\n",
    "print(f\"Log Loss for xgb model 1: {log_loss(y_val_1, xgb_1_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:19:35.595134Z",
     "start_time": "2019-02-27T05:19:35.362633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape: Counter({0.0: 5553, 1.0: 5553})\n"
     ]
    }
   ],
   "source": [
    "# Resampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "rus = RandomUnderSampler()\n",
    "features_res_2, labels_res_2 = rus.fit_sample(features_train, labels_2)\n",
    "print(f\"Resampled dataset shape: {Counter(labels_res_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:19:35.923424Z",
     "start_time": "2019-02-27T05:19:35.919336Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(features_res_2, labels_res_2, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:20:10.797356Z",
     "start_time": "2019-02-27T05:20:10.778972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for lr model 2: 0.9459\n",
      "Log Loss for lr model 2: 0.3436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xulian/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr_2 = LogisticRegression(class_weight = \"balanced\")\n",
    "lr_2.fit(X_train_2, y_train_2)\n",
    "lr_2_pred = lr_2.predict_proba(X_val_2)[:,1]\n",
    "print(f\"ROC AUC for lr model 2: {roc_auc_score(y_val_2, lr_2_pred):.4f}\")\n",
    "print(f\"Log Loss for lr model 2: {log_loss(y_val_2, lr_2_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:20:49.793850Z",
     "start_time": "2019-02-27T05:20:49.781745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for nb model 2: 0.9090\n",
      "Log Loss for nb model 2: 0.3239\n"
     ]
    }
   ],
   "source": [
    "nb_2 = BernoulliNB(fit_prior=True)\n",
    "nb_2.fit(X_train_2, y_train_2)\n",
    "nb_2_pred = nb_2.predict_proba(X_val_2)[:,1]\n",
    "print(f\"ROC AUC for nb model 2: {roc_auc_score(y_val_2, nb_2_pred):.4f}\")\n",
    "print(f\"Log Loss for nb model 2: {log_loss(y_val_2, nb_2_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:21:24.215341Z",
     "start_time": "2019-02-27T05:21:24.166730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for rf model 2: 0.9449\n",
      "Log Loss for rf model 2: 0.5674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xulian/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rf_2 = RandomForestClassifier()\n",
    "rf_2.fit(X_train_2, y_train_2)\n",
    "rf_2_pred = rf_2.predict_proba(X_val_2)[:,1]\n",
    "print(f\"ROC AUC for rf model 2: {roc_auc_score(y_val_2, rf_2_pred):.4f}\")\n",
    "print(f\"Log Loss for rf model 2: {log_loss(y_val_2, rf_2_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T05:23:24.941554Z",
     "start_time": "2019-02-27T05:23:24.907762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for xgb model 2: 0.9554\n",
      "Log Loss for xgb model 2: 0.6883\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model_2 = xgb.XGBClassifier(\n",
    " learning_rate =0.001,\n",
    " n_estimators=9,\n",
    " max_depth=6,\n",
    " min_child_weight=1,\n",
    " subsample=0.3,\n",
    " colsample_bytree=0.5,\n",
    " objective= 'binary:logistic')\n",
    "\n",
    "xgb_2 = model_2.fit(X_train_2, y_train_2)\n",
    "xgb_2_pred = xgb_1.predict_proba(X_val_2)[:,1]\n",
    "print(f\"ROC AUC for xgb model 2: {roc_auc_score(y_val_2, xgb_2_pred):.4f}\")\n",
    "print(f\"Log Loss for xgb model 2: {log_loss(y_val_2, xgb_2_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
